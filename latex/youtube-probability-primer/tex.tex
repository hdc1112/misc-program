% https://latexbase.com/

\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{dsfont}
\usepackage[latin1]{inputenc}

\title{Notes on Youtube's probability primer}
\date{\today}

\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\T}{\mathcal{T}}

\begin{document}
\maketitle

\section{Definition of $\sigma$-algebra}

Given a set $\Omega$, a $\sigma$-algebra $\A$ is a non-empty subset of $2^\Omega$ such that:

\begin{enumerate}
    \item closed under complement, $E\in \A \implies E^c \in \A$ 

    \item closed under countable unions, $E_1\in \A, E_2\in \A \implies E_1\cup E_2\in \A$
\end{enumerate}

\bigbreak

\textbf{Theorem 1} $\Omega\in \A$

Proof: since $\A$ is non-empty, there must exist one element $E\in \A$. Because $\A$ is closed
under complement, so $E^c\in \A$. Because of $\A$ is closed under countable unions, so
$E \cup E^c \in \A$. Hence $\Omega\in A$.
\bigbreak

\textbf{Theorem 2} $\phi \in \A$

Proof: because $\Omega \in \A$ and $\A$ is closed under complement, so $\Omega^c=\phi\in \A$.
\bigbreak

\textbf{Theorem 3} $\A$ is closed under countable intersections.

Proof: $\bigcap\limits_{i=1}^{\infty} E_i = (\bigcup\limits_{i=1}^{\infty} E_i^c)^c$
From this equation we can see that $\A$ is closed under intersections.

\section{Definition of generated $\sigma$-algebra}

Given a $\C\subseteq 2^\Omega$, the generated $\sigma$-algebra $\C$, or $\sigma(\C)$, is the smallest
$\sigma$-algebra containing $\C$.
Or, $\sigma(\C)=\bigcap\limits_{\A\ is\ \sigma-algebra\ on\ \Omega,\ \C\subseteq \A} \A$.
\bigbreak

\textbf{Theorem 4} $\sigma(\C)$ always exists

Proof: $2^\Omega$ is a $\sigma$-algebra, and any intersections of $\sigma$-algebra is also
$\sigma$-algebra (needed by the mathematical ``smallest" definition). (These two can be easily proved.)
\bigbreak

Example of $\sigma$-algebra:
\begin{enumerate}
    \item $\A=\{\phi, \Omega\}$
    \item $\A=\{\phi,E, E^c, \Omega\}$
    \item if $\Omega = \mathbb{R}$, the Borel $\sigma$-algebra $\B(\mathbb{R})=\sigma(\T)$ where
        $\T=\{open\ sets\ of\ \mathbb{R}\}$.
\end{enumerate}

\section{Definition of measure}

A measure $\mu$ on $\Omega$ with $\sigma$-algebra $\A$ is a function
$\mu: \A \rightarrow [0,\infty]$ such that:

\begin{enumerate}
    \item $\mu(\phi) = 0$
    \item ``countable additivity" $\mu(\bigcup\limits_{i=1}^{\infty} E_i)=\sum\limits_{i=1}^{\infty} \mu(E_i)$ for any pairwise disjoint sets $E_1, E_2, ... \in \A$ 
\end{enumerate}

\section{Definition of probability measure}

A probability measure is a measure $P$ such that $P(\Omega) = 1$. (Notice that $\Omega\in \A$). Also known as Kolmogorov's axioms.

\section{Example of measures}

\begin{enumerate}
\item $\Omega=\{1,2,...,n\}$, $\A=2^\Omega$, 
$P(\{k\})=P(k)=1/n,\forall k\in\Omega$. Uniform distribution. We do
not need to define other element $E\in\A$ because of countable
additivity.

\item Countable infinite. $\Omega=\{1,2,3,...\}$, $\A=2^\Omega$,
$P(\{k\})=P(k)=\\ probability\ it\ takes\ k\ coinflips\ to\ get\ heads
=\alpha * (1-\alpha)^{k-1}=1/2 * (1 - 1/2)^{k-1}$. Geometric distribution.

\item Uncountable infinite. $\Omega=[0,\infty)$, $\A=\B(\Omega)$,
$P([0,x))=1-e^{-x}, \forall x>0$. Exponential distribution. Note
that $P({x})=0,\forall x\ge 0$ because this is a continuous distribution.

\item Lebesgue measure. $\Omega=\mathbb{R}$, $\A=\B(\mathbb{R})$,
$\mu((a,b)) = b - a,\forall a,b\in\mathbb{R},a<b$. 
\end{enumerate}

\section{Basic properties of measures}

Let $(\Omega,\A,\mu)$ be a measure space.

\begin{enumerate}
    \item Monoticity: If $E,F\in\A$ and $E\subset F$, then $\mu(E)\le\mu(F)$
    \item Subadditivity: If $E_1,E_2,...\in\A$, then
        $\mu(\bigcup\limits_{i=1}^{\infty} E_i)\le \sum\limits_{i=1}^{\infty} \mu(E_i)$

    \item Continuity from below: If $E_1,E_2,...\in\A$ and $E_1\subset E_2\subset ...$,
        then $\mu(\bigcup\limits_{i=1}^{\infty} E_i)=\lim\limits_{i\rightarrow\infty} \mu(E_i)$

    \item Continuity from above: If $E_1,E_2,...\in\A$ and $E_1\supset E_2\supset ...$
        and $\mu(E_1)<\infty$, then
        $\mu(\bigcap\limits_{i=1}^{\infty} E_i)=\lim\limits_{i\rightarrow\infty} \mu(E_i)$
\end{enumerate}

\section{Facts}

Let $(\Omega,\A,P)$ be probability measure space with $E,F,E_i\in\A$.

\begin{enumerate}
    \item $P(E\cup F)=P(E)+P(F)$ if $E\cap F=\phi$
    \item $P(E\cup F)=P(E)+P(F)-P(E\cap F)$
    \item $P(E)=1-P(E^c)$
    \item $P(E\cap F^c)=P(E) - P(E\cap F)$
    \item (Inclusion-Exclusion formula)
        $P(\bigcup\limits_{i=1}^{n} E_i)=\sum\limits_{i} P(E_i) - \sum\limits_{i<j} P(E_i\cap E_j) 
        + \sum\limits_{i<j<k} P(E_i\cap E_j\cap E_k) - ... + (-1)^{n+1}P(E_1\cap E_2\cap ... \cap E_n)$

    \item $P(\bigcup\limits_{i=1}^{n} E_i)\le \sum\limits_{i=1}^{n} P(E_i)$ and
          $P(\bigcup\limits_{i=1}^{\infty} E_i)\le \sum\limits_{i=1}^{\infty} P(E_i)$
\end{enumerate}

\section {$\mathbb{R}$ and $\B(\mathbb{R})$}

Definition: A Borel measure on $\mathbb{R}$ is a measure on $(\mathbb{R},\B(\mathbb{R}))$.

Definition: A CDF (cumulative distribution function) is a function $F:\mathbb{R}\rightarrow\mathbb{R}$
such that 
\begin{enumerate}
    \item $F$ is nondecreasing $(x\le y \Rightarrow F(x)\le F(y))$
    \item $F$ is right continuous $(\lim\limits_{x\searrow a} F(x)=F(a))$
    \item $\lim\limits_{x\rightarrow\infty} F(x)=1$
    \item $\lim\limits_{x\rightarrow -\infty} F(x)=0$
\end{enumerate}

\bigbreak
\textbf{Theorem}: There is an equivalence between CDFs and Borel probability measures.

\begin{enumerate}
    \item If $F$ is a CDF then there is a unique Borel probability measure on $\mathbb{R}$
such that $P((-\infty,x]))=F(x), \forall x\in\mathbb{R}$

\item If $P$ is a Borel probability measure on $\mathbb{R}$ then there is a unique CDF $F$ such
that $F(x)=P((-\infty,x]), \forall x\in\mathbb{R}$

\end{enumerate}

\section{References on measure theory}

Real analysis (Undergrad)
\begin{enumerate}
\item Rudin's ``Principles of Mathematical Analysis"
\end{enumerate}

Probability (grad)

\begin{enumerate}
    \item Jacod \& Protter "Probability Essentials"
    \item Durrett ``Probability Theory and Examples"
    \item Grimmett \& Stirzaker ``Probability \& Random Processes"
\end{enumerate}

Real analysis (Grad)

\begin{enumerate}
    \item Fdlord's ``Real Analysis"
    \item Rudin ``Real \& Complex Analysis"
\end{enumerate}

\section{Summary}

Definition: A $\sigma-algebra$ on $\Omega$ is a collection $\A\subset 2^\Omega$ such that
\begin{enumerate}
    \item $E\in\A \Rightarrow E^c\in\A$
    \item $E_1,E_2,...\in\A\Rightarrow \bigcup\limits_{i=1}^{\infty} E_i\in\A$ Also closed under finite unions
\end{enumerate}

Definition: A probability measure $P$ on $(\Omega,\A)$ is a function $P:\A\rightarrow [0,1]$
such that
\begin{enumerate}
    \item $P(\phi)=0$, and $P(\Omega)=1$.
    \item $P(\bigcup\limits_{i=1}^{\infty} E_i)=\sum\limits_{i=1}^{\infty} P(E_i)$
        for any $E_1,E_2,...\in\A$ pairwise disjoint set
\end{enumerate}

Remarks:
\begin{enumerate}
    \item $P(E\cup F)=P(E)+P(F)-P(E\cap F), E,F\in\A$
    \item $P(E)=1-P(E^c), \forall E\in \A$
    \item $E\subset F$ then $P(E)\le P(F)$
    \item If $E_1,E_2,...\in\A$ then $P(\bigcup\limits_{i=1}^{\infty} E_i)\le
        \sum\limits_{i=1}^{\infty} P(E_i)$

    \item If $E_1\subset E_2\subset... $ then $P(\bigcup\limits_{i=1}^{\infty} E_i)
        =\lim\limits_{i\rightarrow \infty} P(E_i)$

    \item If $E_1\supset E_2\supset...$ then $P(\bigcap\limits_{i=1}^{\infty} E_i)
        =\lim\limits_{i\rightarrow \infty} P(E_i)$

\end{enumerate}

Definition: A CDF is a function $F:\mathbb{R}\rightarrow\mathbb{R}$ such that
\begin{enumerate}
    \item $x\le y \Rightarrow F(x)\le F(y) (x,y\in \mathbb{R})$
    \item $\lim\limits_{x\searrow a} F(x) = F(a)$
    \item $\lim\limits_{x\rightarrow \infty} F(x) = 1$
    \item $\lim\limits_{x\rightarrow -\infty} F(x)=0$
\end{enumerate}

Theorem: $F(x)=P((-\infty,x])$ defines a one-to-one mapping from CDF function $F$
to Borel probability measure function $P$. That is to say, if
we have a CDF function $F$, then we have a probability measure $P$, and vice versa.
This simplies lots of things, and deserves a theorem.


\section{Conditional Probability and Independence}

Notation: hidden $(\Omega,\A)$ and $P(E)$.

Terminology: event = measurable set = set in $\A$. sample space = $\Omega$. 
\bigbreak

Definition: If $P(B)>0$, then $P(A|B)=\frac{P(A\cap B)}{P(B)}$.
\bigbreak

Definition: Events $A, B$ are independent if $P(A\cap B) = P(A) P(B)$.
\bigbreak

Theorem: Suppose $P(B)>0$, then $A,B$ are indep. iff $P(A|B)=P(A)$

Proof: (trivial, by def. of $|$ and def. of indep.)
\bigbreak

Definition: $A_1,A_2,...,A_n$ are mutually independent if $\forall S\subseteq \{1,2,...,n\}$,
$P(\bigcap\limits_{i\in S} A_i) = \prod\limits_{i\in S} P(A_i)$

Remark: mutually independence $\Rightarrow$ pairwise indepenjdence

Remark: pairwise independence $\nRightarrow$ mutually independence

Remark: One interesting picture to show $n$ mutually independent sets (PP 2.2).
\bigbreak

Definition: $A_1, A_2,...$ are MI if $\forall\ finite\ S\in \{1,2,...\}$ (like above).
\bigbreak

Definition: Events $A,B$ are conditionally independent given $C$ if
$P(A\cap B | C) = P(A|C) P (B|C)$.

Remark: $A,B$ are conditionally independent given $C$ does not always mean that $A$ and $B$
are independent.
\bigbreak

Definition: $A_1, A_2,...,A_n$ are CI given $C$ if $\forall\ finite\ S\in \{1,2,...,n\}$,
$P(\bigcap\limits_{i\in S} A_i|C)=\prod\limits_{i\in S} P(A_i|C)$. ($P(C) > 0$ for all conditional
independence definition).

\section{Bayes' rule}

Remind: $P(A\cap B) = P(A|B)P(B)$ if $P(B) > 0$ (from def. of $|$)

Rewrite: $P(B\cap A) = P(B|A) P(A)$ if $P(A) > 0$ (from def. of $|$)
\bigbreak

\textbf{The Bayes' rule}: $P(B|A)=\frac{P(A|B)P(B)}{P(A)}$ if $P(A)>0$ and $P(B)>0$

\section{Chain rule}

\textbf{Chain rule}: If $A_1,A_2,...,A_n$ and $P(A_1\cap A_2...\cap A_{n-1}) > 0$,
then $P(A_1\cap A_2...\cap A_n) = P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)...P(A_n|A_1\cap ... \cap A_{n-1})$.

Proof: (trivial, induction over $n$)

Remark: $P(A_1\cap A_2\cap ... \cap A_{n-1})>0$ already implies that $\forall s\in \{2,...n\}$,
$P(A_1\cap A_2\cap ...\cap A_{s-1}) > 0$.

\section{Partition rule}

Definition: A partition of $\Omega$ is a finite or countable infinite collection
of $\{B_i\}\subset 2^{\Omega}$, such that
\begin{enumerate}
    \item $\bigcup\limits_i B_i = \Omega$
    \item $B_i\cap B_j = \phi, i\neq j$
\end{enumerate}

\textbf{Partition rule}: $P(A) = \sum\limits_i P(A\cap B_i)$ for any paritition $\{B_i\}$ of $\Omega$.

Proof: $A=A\cap \Omega=A\cap (\bigcup\limits_i B_i)=\bigcup\limits_i (A\cap B_i)$, and
$(A\cap B_i)\cap (A\cap B_j)=\phi, i\neq j$, so because of additivity of measure,
we have $P(A) = \sum\limits_i P(A\cap B_i)$.

\section{Conditional measure}

Theorem: If $P(B)>0$, then $Q(A)=P(A|B)$ defines a probability measure $Q$ (conditional
probability measure given $B$.

Proof: (trivial, use Partition rule)

\section{Random Variables}

Definition: Given $(\Omega, \A, P)$, a random variable is a function $X:\Omega \rightarrow \mathbb{R}$
such that $\{w\in \Omega: X(w)\le x\} \in \A$ $\forall x\in \mathbb{R}$
\bigbreak

Remarks
\begin{enumerate}
    \item $X$ is called a measurable function if $X$ satisfies that $\{w\in \Omega: X(w)\le x\} \in \A$ $\forall x\in \mathbb{R}$
    \item Notation: capital letters $X, Y$ for random variables, little letters $x,y$ for values
    \item Notation: $\{X\le x\}=\{w\in\Omega:X(w)\le x\}$; $\{...X...\}=\{w\in\Omega:...X(w)...\}$
    \item Notation: $P(X\le x)=P(\{X\le x\})$; $P(...)=P(\{...\})$
\end{enumerate}
\bigbreak

Definition (ext.): The CDF of a random variable $X$ is the function $F:\mathbb{R}\rightarrow [0,1]$
such that $F(x)=P(X\le x)$ (this definition fits the old CDF definition above, proof trivial)

Warning: before this, we have the definition of probability measure, and the definition
of CDF. now, this new function $F(x)=P(X\le x)$, which is defined on top of probability measure
and random variable,
is actually a CDF because it fits the old CDF definition. 
\bigbreak

Definition (ext.): The distribution of $X$ is the function $P^X: \B(\mathbb{R})\rightarrow \mathbb{R}$
such that $P^{X}(A)=P(X\in A)$ where $A\in \B(\mathbb{R})$. Proof that $P^X$ is a
probability measure function is trivial.
\bigbreak

Warning: $P(X\in A)=P(\{w\in\Omega | X(w)\in A\})$ where $A\in \mathcal{B}(\mathbb{R})$ (informally,
treat it as $A\subseteq \mathbb{R}$)
\bigbreak

%Useful link: \url{http://math.arizona.edu/~tgk/mc/prob_background.pdf}
%\bigbreak

Claim: $P^X$ is the probability measure induced by $F$ (because previously we mentioned that
there is a one-to-one mapping between CDF and probability measure, we need to prove
that the $P^X$ is exactly the same with that probability measure)

Proof: $Q((-\infty,x])=F(x)=P(X\le x)=P(X\in(-\infty,x])=P^{X}((-\infty,x])$
this implies that $Q=P^X$.
\bigbreak

\textbf{Summary} After defining random variable $X$, we defined two new functions on top of it:
\begin{enumerate}
    \item $P^X(A) = P(X\in A), A\in\B(\mathbb{R})$. It turns out this is a probability measure function.
        We call it ``the distribution of random variable $X$".
    \item $F(x) = P(X\le x), x\in \mathbb{R}$. It turns out this is a CDF function.
        We call it ``CDF of the random variable $X$".
\end{enumerate}
And finally it turns out they form a pair, and each of them can be induced from the other.

\section{Types of Random Variables}

Definition: A random variable $X$ is discrete if $X(\Omega)=\{X(w):w\in\Omega\}$ is countable.
e.g., $X(\Omega)=\{x_1, x_2,...\}$.
\bigbreak

Definition: A random variable $X$ has a density $f$ if $F(x)=\int\limits_{-\infty}^x f(u)du, \forall x\in \mathbb{R}$ 
(for some integrable $f:\mathbb{R}\rightarrow [0,\infty)$). (so density function is defined after CDF)
\bigbreak

Advanced part below.

Let $Q=P^X$. Let $J=\{x\in\mathbb{R}:Q(\{x\})>0\}$. Let $Q_d(A)=Q(A\cap J)$.
Let $Q_c(A) = Q(A) - Q(A\cap J)$. So $Q= Q_d + Q_c$.

$Q_d$ is the discrete part of this measure $Q$,
and $Q_c$ is the continuous part of this measure $Q$.

$Q_c=Q_{ac} + Q_{sc}$ (by R-N theorem) where $Q_{ac}((-\infty,x])=\int\limits_{-\infty}^{x} f(u)du$
for some $f:\mathbb{R}\rightarrow [0,\infty)$.

Claim: $X$ is discrete $\Rightarrow$ $Q=Q_d$; $X$ has density $\Leftrightarrow$ $Q=Q_{ac}$. (Proof omitted).

Warning: beware of the singular part! $Q=Q_c \nRightarrow X$ has a density.
One counter-example is: cantor function as CDF, which $Q_d=0,Q_{ac}=0$
\bigbreak

Remark: need to learn mathematics..

\section{Discrete Random Variable}

Recall that $X$ is a random variable if $X(\Omega)$ is countable.
\bigbreak

Definition: The probability mass function (pmf) of a discrete random variable
is the function $p:\mathbb{R}\rightarrow [0,1]$ such that $p(x)=P(X=x)$.

Remarks:
$P(X\in A)=\sum\limits_{x\in A\cap S} p(x)$ where $S=X(\Omega)$. 

Reasons: Because $P(X\in A)=P^{X}(A)=P^{X}(A\cap S)+P^{X}(A\cap S^c)$.
        But $S^c=\phi$, so $P^{X}(A\cap S^c)=0$. So $P^{X}(A)=P^{X}(A\cap S)=\sum\limits_{x\in A\cap S}P(X=x)=
        \sum\limits_{x\in A\cap S} p(x)$.
        
Lemma: $P(X\in \mathbb{R})=\sum\limits_{x\in S} p(x)=1$.
\bigbreak

Notation: $X\sim p$ or $X\sim F$ (it is either pmf (or density) or CDF) or $X\sim Q$ where $Q$ is distribution of random variable.
\bigbreak

Examples:
\begin{enumerate}
    \item $X\sim Bernoulli(\alpha)$, $\alpha \in[0,1]$, $p(1)=\alpha$, $p(0)=1-\alpha$.
        
    \item $X\sim Binomial(n,\alpha)$, $\alpha \in [0,1]$, $p(k)=\binom{n}{k}\alpha^k (1-\alpha)^{n-k}$ where
        $k\in\{0,1,2,...,n\}$.
        
    \item $X\sim Geometric(\alpha)$, $\alpha\in [0,1]$, $p(k)=(1-\alpha)^{k-1}\alpha$ where $k\in\{1,2,...\}$.

    \item $X\sim Poisson(\lambda),\lambda\ge 0$, $p(k)=e^{-\lambda}\frac{\lambda^{k}}{k!}$ where $k\in\{0,1,2,...\}$.
\end{enumerate}

\section{Random variables with densities}

Recall CDF $F(x)=P(X\le x)=\int\limits_{-\infty}^{x} f(u)du$
\bigbreak

Notation:
\begin{enumerate}
    \item $f$ is the probability density function (pdf) of $X$, and 
        we write $X\sim f$.

    \item $P$ for probability measure, $p$ for pmf, $f$ for pdf. Some places use $p$ for pdf.

    \item Indicator function of $A$.
        \begin{equation}
            I_{A}(x)=\begin{cases}
                1, & \text{if $x\in A$}. \\
                0, & \text{otherwise}.
            \end{cases}
        \end{equation}
\end{enumerate}
\bigbreak

Examples:
\begin{enumerate}
    \item $X\sim Uniform(a,b), a<b$, $f(x)=\frac{1}{b-a}, x\in[a,b]$, and $f(x)=0$ otherwise.

    \item $X\sim Exponential(\lambda),\lambda>0$, $f(x)=\lambda e^{-\lambda x},x\ge 0$, and $f(x)=0$ otherwise. ``memoryless" property.

    \item $X\sim Beta(\alpha,\beta),\alpha>0,\beta>0$, $f(x)=\frac{x^{\alpha - 1}(1-x)^{\beta - 1}}{B(\alpha,\beta)},x\in[0,1]$,
        and $f(x)=0$ otherwise.

    \item $X\sim Normal(\mu,\sigma^2),\mu\in \mathbb{R},\sigma^2>0$, 
        $f(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}},x\in\mathbb{R}$. a.k.a. Gaussian distribution.
\end{enumerate}

\section{Expectation for discrete random variables}

Intuition: expectation is ``average value".

Let $\mathfrak{X}=X(\Omega)$.

Definition: the expectation of a discrete random variable with pmf $p$ is
$E(X)=\sum\limits_{x\in\mathfrak{X}} xp(x)$ when this sum is ``well-defined".
Otherwise, expectation does not exist.
\bigbreak

Examples:
$X\sim Bernoulli(\alpha),\mathfrak{X}=\{0,1\}$.
$E(X)=0*p(0)+1*p(1)=\alpha$
\bigbreak

What is ``well-defined"?

One example of not Well-defined: $1-1+1-1+1-1....$ is not well-defined.
\bigbreak

Definition: Let $a_1,a_2,...\in\mathbb{R}$, $\sum\limits_{i=1}^{\infty}a_i=\sum\limits_{i:a_i\ge 0}a_i
+\sum\limits_{i:a_i<0}a_i=b+c$. Then $\sum\limits_{i=1}^{\infty}a_i$ is well-defined if 
either $b$ or $c$ is finite.
\bigbreak

$E(X)$ may exist and could be $\infty$. Example:
We know that $c=\sum\limits_{k=1}^{\infty}\frac{1}{k^2}$ where $c$ is a proved constant and
$c<\infty$. We can construct a pmf with $p(k)=\frac{1}{ck^2}$ for $k\in\{1,2,...\}$.
This is good because $\sum\limits_{k=1}^{\infty}p(k)=1$.
But $E(X)=\sum\limits_{k=1}^{\infty}\frac{k}{ck^2}=\sum\limits_{k=1}^{\infty}\frac{1}{ck}=\infty=\infty$.
\bigbreak

$E(X)$ might not exist.
\begin{equation}
    p(k)=\begin{cases}
        \frac{1}{2ck^2}, & \text{if $k\in \mathds{Z}$, $k \neq 0$} \\
        0, & \text{otherwise}
    \end{cases}
\end{equation}

$E(X)=\sum\limits_{k\in\mathds{Z}}kp(k)=\infty-\infty=undefined$.

\section{Expectation for random variables with densities}

Definition: The expectation of a random variable $X$ with density $f$
is $E(X)=\int\limits_{-\infty}^{\infty}xf(x)dx$ when this integral is well defined.
Otherwise, the expectation does not exist.

Example: $X\sim Uniform(a,b)$ $E(X)=\int\limits_{a}^{b}\frac{x}{b-a}dx=\frac{a+b}{2}$
\bigbreak

What is ``Well-defined"?: $a=\int\limits_{-\infty}^{0}xf(x)dx$, $b=\int\limits_{0}^{\infty}xf(x)dx$
because $f(x)>=0$. The integral of $\int\limits_{-\infty}^{\infty}xf(x)dx$ is well-defined if either
$a$ or $b$ is finite.
\bigbreak

So $E(X)$ may exist and be $\infty$, and might not exist at all.

Example: $X\sim Cauchy$ or $f(x)=\frac{1}{\pi(1+x^2)}$ then
$E(X)=\int\limits_{-\infty}^{\infty}\frac{x}{\pi (1+x^2)}=\int\limits_{-\infty}^{-1} +
\int\limits_{-1}^{1}+\int\limits_{1}^{\infty}=-\infty + ... + \infty = undefined$.
Because $\int\limits_{1}^{a}\frac{x}{1+x^2}\ge\int\limits_{1}^{a}\frac{x}{2x^2}=\frac{1}{2}lna$
so it goes to $\infty$ if $a\rightarrow\infty$.

\section{Expectation rule}

Fact: $g(X)$ is a random variable if $X$ is a random variable and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a measurable function.

Notation: $EX=E(X)$.

Remark: $g(X)$ only means a random variable, it does not mean mapping from $X$ to something. That is to say,
$g(X)$ is a whole symbol and do not divide it into function $g$ and parameter $X$.

Theorem (Expectation rule): if $X$ is a random variable and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a (measurable) function,
then
\begin{enumerate}
    \item $Eg(X)=\sum\limits_{x\in\mathfrak{X}}g(x)p(x)$ if $X$ is discrete with pmf $p$, and if well-defined,
    \item $Eg(X)=\int\limits_{-\infty}^{\infty}g(x)f(x)$ if $X$ has density with pdf $f$, and if well-defined
\end{enumerate}

\bigbreak

Notation: $X:\Omega\rightarrow\mathbb{R}$ so $X(\Omega)$ means the set of all possible values in $X(w)$,
$g(X):\mathbb{R}\rightarrow\mathbb{R}$ so $g(\mathfrak{X})$ means the set all possible values in $g(x)$.

\bigbreak
Suppose $Y=g(X)$

Proof of discrete case:

$EY=\sum\limits_{y\in g(\mathfrak{X})} y P(Y=y) \\ 
= \sum\limits_{y\in g(\mathfrak{X})} y \sum\limits_{x:g(x)=y} p(x) \\ 
= \sum\limits_{y\in g(\mathfrak{X})}\sum\limits_{x:g(x)=y} y p(x) \\
= \sum\limits_{y\in g(\mathfrak{X})} \sum\limits_{x:g(x)=y} g(x) p(x) \\
= \sum\limits_{x\in \mathfrak{X}} g(x)p(x)$

\bigbreak
Proof of density case:

The key step is $f_Y(y) dy = \int\limits_{x=-\infty,g(x)=y}^{x=\infty} f_X(x) dx$

$EY = \int\limits_{y=-\infty}^{y=\infty} y f_{Y}(y) dy \\ 
= \int\limits_{y=-\infty}^{y=\infty} y \int\limits_{x=-\infty,g(x)=y}^{x=\infty} f_X(x)dx \\ 
= \int\limits_{y=-\infty}^{y=\infty} \int\limits_{x=-\infty,g(x)=y}^{x=\infty} y f_X(x)dx \\
= \int\limits_{y=-\infty}^{y=\infty} \int\limits_{x=-\infty,g(x)=y}^{x=\infty} g(x) f_X(x)dx \\
= \int\limits_{x=-\infty}^{x=\infty} g(x) f_X(x)dx$

\section{Properties of Expectation}

Theorem: Suppose $X,Y$ are random variables and $E|X|<\infty,E|Y|<\infty$,
\begin{enumerate}
    \item $E(X)=a$ if $X(w)=a, \forall w\in\Omega$
    \item $E(aX)=aE(X), \forall a\in\mathbb{R}$
    \item $E(X+Y)=E(X)+E(Y)$, proof needs future knowledge, on hold.
    \item if $X\ge 0$, then $E(X)\ge 0$. ($X\ge 0$ means $X(w)\ge 0,\forall w\in\Omega$)
    \item if $X\le Y$, then $E(X)\le E(Y)$. ($X\le Y$ means $X(w)\le Y(w),\forall w\in\Omega$)
    \item $E(I_A(X))=P(X\in A)$. ($I_A(x)$ is defined above)
\end{enumerate}
The first three together are called ``linearity of expectation".
The fourth and fifth together are called ``order preserving property of expectation".
The 1st, 2nd, 4th, 5th are easy to prove. The 3rd needs knowledge from joint
distribution, so the 3rd will be proved in the future.

\bigbreak
Theorem: if $X_1,X_2,...$ is a sequence of random variables and $X_i\ge 0,\forall i\ge 1$,
then $E(\sum\limits_{i=1}^{\infty}X_i)=\sum\limits_{i=1}^{\infty}E(X_i)$. Proof omitted because
of advanced measure theory knowledge.

\bigbreak
Proof of 6th in density case (discrete case is trivial, which can be found in 3rd CLRS):
Let $Q(A)=EI_A(X)$. We claim that $Q(A)$ is a measure function. Because it
satisfies the following three properties.
\begin{enumerate}
    \item $Q(A)\ge 0$. $I_A(X)$ can only take $0,1$, which are both non-negative values. Based on the 4th
        theorem above, so $Q(A)=EI_A(X)\ge 0$
    \item $Q(\phi)=0$. $I_{\phi}(X)$ now only takes one value $0$, so based on the 1st theorem above,
        $Q(\phi)=E(I_{\phi}(X))=0$.
    \item $Q(\bigcup\limits_{i=1}^{\infty}A_i)=EI_{\bigcup\limits_{i=1}^{\infty}A_i}(X)
        =(indicator\ function\ property)E\sum\limits_{i=1}^{\infty}I_{A_i}(X)
        =(theorem\ above)\sum\limits_{i=1}^{\infty}EI_{A_i}(X)
        =\sum\limits_{i=1}^{\infty}Q(A_i)$, for mutually disjoint sets $A_i$.
\end{enumerate}

$Q((-\infty,a])=EI_{(-\infty,a]}(X)=\int\limits_{-\infty}^{\infty}I_{(-\infty,a]}(x)f(x)dx
=\int\limits_{-\infty}^{a}f(x)dx=F(a)=P(X\le a)=P(X\in(-\infty,a])=P^{X}((-\infty,a])$

Because there is a one-to-one mapping between CDF and probabilitistic measure function,
and $Q(A)$ is a probabilistic measure function,
so $Q=P^X$. So $Q(A)=P^X(A)=P(X\in A)$. So $EI_A(X)=Q(A)=P(X\in A)$.

\section{Mean, Variance, and Moments}

Let $X$ be a random variable

Definition: The mean $\mu(X)$ is $EX$.

Definition: The variance $\sigma^2(X)$, or $Var(X)$, is $E((X-EX)^2)$.

Definition: The standard deviation of $\sigma(X)$ is $\sqrt{\sigma^2(X)}$.

Definition: The $k^{th}$ moment where $k\in \{1,2,3,...\}$,
$m_k(X)$ is $E(X^k)$.

Definition: The $k^{th}$ central moment is $E((X-EX)^k)$.

Proposition: $\sigma^2(X)=E(X^2)-(EX)^2$.

\section{Multiple discrete random variables}

Definition: given $(\Omega,\A, P)$,
a random vector is a measurable fuction $X:\Omega\rightarrow \mathbb{R}^d$ where $d\in\{1,2,...\}$.
Notice that $X\in\mathbb{R}^d$.

Definition: a discrete random vector $X\in \mathbb{R}^d$ is a random vector
whose $X(\Omega)$ is countable. (Just like one dimensional case).

Definition: The joint pmf of a discrete random vector $X\in\mathbb{R}^d$
is the function $p:\mathbb{R}^d\rightarrow[0,1]$ such that 
$p(x)=P(X=x),\forall x\in\mathbb{R}^d$.

Notation: $X=(X_1,...,X_d)$, and $x=(x_1,...x_d)$.
($X=x$ means $X_i=x_i$ for $i\in\{1,...,d\}$.)
$p(x)=p(x_1,...,x_d)$.
$p_X(x)=p(x)$.

Remark: $P(X\in A)=\sum\limits_{x\in(\mathfrak{X}\cap A)} p(X=x)$ where $\mathfrak{X}=X(\Omega)$

Remark: $g(X)$ is also a random vector if $X\in\mathbb{R}^d$ is a random vector
and $g:\mathbb{R}^d\rightarrow\mathbb{R}^k$ is a measurable function.

Proposition: $Eg(X)=\sum\limits_{x\in\mathfrak{X}} g(x)p(x)$ for
any (measurable) $g:\mathbb{R}^d\rightarrow\mathbb{R}^k$
such that this sum is well-defined.

\section{Marginals and conditionals}

Fix $(X,Y)\in\mathbb{R}^2$ with probability distribution $p(x,y)=P(X=x,Y=y)$

Definition: The marginal pmf of $X$ is $p_X(x)=P(X=x)$ .

Proposition: $p_X(x)=\sum\limits_{y\in\mathfrak{Y}} p(x,y)$ where $\mathfrak{Y}=Y(\Omega)$.

Proof: $p_X(x) = P(X=x) = P(\{w\in\Omega:X(w)=x\}) = P(\bigcup\limits_{y\in\mathfrak{Y}} \{w\in\Omega:X(w)=x,Y(w)=y\}
=\sum\limits_{y\in\mathfrak{Y}}P(X=x,Y=y)=\sum\limits_{y\in\mathfrak{Y}}p(x,y)$

Notation: $p(x)=p_X(x)$, $p(y)=p_Y(y)$.

Definition: The conditional pmf of $X$ given $Y=y$
is $p(x|y)=P(X=x|Y=y)$ when $P(Y=y)>0$.

Remark: $P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}=\frac{p(x,y)}{p_Y(y)}$

Definition: The conditional expectation of $X$ given $Y=y$ (when $p(y)>0$),
is $E(X|Y=y)=\sum\limits_{x\in\mathfrak{X}} x p(x|y)$,
when this sum is well defined.

Remark: $E(X|Y=y)$ is a number.
But $E(X|Y)$ is a random variable that depends on $Y$.

\section{Multiple random variables with densities}

Notation: for $g:\mathbb{R}^k\rightarrow\mathbb{R}$,
$\int\limits_{\mathbb{R}^k} g(x)dx = \int\limits_{-\infty}^{\infty}...\int\limits_{-\infty}^{\infty}g(x_1,...,x_k)dx_1...dx_k$

For $A\in\B(\mathbb{R}^k)$, $\int\limits_{A}g(x)dx=\int\limits_{\mathbb{R}^k}I_A(x)g(x)dx$

Remark: Riemann integral is different from Lebesgue integral. For function $f:[0,1]\rightarrow\{0,1\}$ where if $x$ is rational,
then $f(x)=1$, otherwise $f(x)=0$, then this function is not Riemann integrable, but it is Lebesgue integrable.

Definition: A random variable $X\in\mathbb{R}^d$ has a density or pdf $f$ if
$P(X\in A)=\int\limits_Af(x)dx,\forall A$ where $f:\mathbb{R}^d\rightarrow[0,+\infty)$ is a measurable function.

For $X\in\mathbb{R}^d$, $Eg(X)=\int\limits_{\mathbb{R}^d}g(x)f(x)dx$ when well-defined.

Fix $(X,Y)\in\mathbb{R}^2$ with joint pdf $f$.
$f_X(x)=\int\limits_{R}f(x,y)dy$, $f(x|y)=\frac{f(x,y)}{f_Y(y)}$ (when $f(y)>0$).

$E(X|Y=y)=\int\limits_{\mathbb{R}}xf(x|y)dx$ (when well-defined).

\end{document}
